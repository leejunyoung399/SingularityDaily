# AI 지능체가 완전한 신약 개발 과정을 자율적으로 설계하고 실행할 수 있는가 - 지능원 커뮤니티

**원제목:** AI智能体是否可以自主设计，并实施完整的药物发现流程 - 智源社区

**요약:** 伝統的な医薬品開発は、高コストで時間がかかるウェット実験プロセスに大きく依存している。AI駆動のシステムは、高度な計算方法によって、このプロセスを大幅に加速し、効率を高めることが期待される。現在の研究は、意思決定を支援し、実験の負担を軽減するための予測モデルの開発に重点を置いているが、より将来的な方向としては、モデルを自律的に設計し、文献レビューを行い、実験経路を選択し、医薬品開発プロセスの全体を通して戦略的決定を行うことができるAIエージェントシステムの構築である。

科学的推論と実験のエージェントシステムに対する関心が高まっているにもかかわらず、医薬品発見タスクに特化したエージェントベンチマークは依然として不足している。既存のベンチマークの多くは、特定の種類の予測能力のみを評価しており、システムの自律的決定、コード生成、または自動実行能力を測定することは困難である。一方、自律能力を対象とした一般的なベンチマークの中には、医薬品開発に関する背景知識が不足しているものもある。

2025年4月28日、Deep OriginチームはarXivに「Can AI Agents Design and Implement Drug Discovery Pipelines?」という論文を発表し、AIエージェントシステムの複雑な意思決定タスクにおける総合能力を評価するためのDO Challengeベンチマークを提案した。このタスクは仮想スクリーニングの状況を模倣しており、システムは制限された計算資源の下で、効率的な戦略を自律的に開発、実装、実行し、数百万の分子構造の中から潜在的な化合物を特定し、多目的のトレードオフの中で化学空間を探求し、モデリング方法を選択し、資源を合理的に配分することを要求する。

背景
大規模言語モデル（LLMs）駆動の自律型エージェントシステムの急速な発展に伴い、医薬品発見への応用可能性が注目されており、研究開発コストの大幅な削減、期間の短縮、成功率の向上に繋がる可能性がある。しかし、このようなシステムを評価するための標準化されたベンチマークは、明らかに遅れている。Therapeutics Data Commons（TDC）、DrugOOD、CARAなどの既存のベンチマークは、ターゲットの特定から汎化予測までの重要なタスクを網羅しているが、多くの場合、これらのタスクを分離しており、真のエージェントに必要な総合能力を評価することはできない。同様に、GuacaMol、MoleculeNet、MolGymなどは、分子設計や性質予測といった単一の能力に焦点を当てており、プロセス全体の総合的な反映を欠いている。総じて、これらのベンチマークは重要な進歩を遂げており、具体的な能力の評価において価値があるものの、依然として孤立したタスクや単一のスキルに焦点を当てているものが多く、資源の制限、アノテーションコストの高騰、多目的最適化など、医薬品発見においてエージェントが直面する複雑な現実を正確に反映することは難しい。

本研究は、医薬品発見におけるAI自律型エージェントシステムの発展の可能性に着目し、仮想スクリーニングのシナリオにおけるAIシステムの総合能力を評価するためのDO Challengeベンチマークを提案する。従来のベンチマークとは異なり、このチャレンジでは、エージェントが資源制約のある条件下で、独立して戦略を策定し実行し、化学空間を探求し、モデルを選択し、資源を管理して、最適な分子構造を特定することを要求する。さらに、複数のLLMで構成される汎用エージェントシステムであるDeep Thoughtの詳細な報告を行い、制限時間付きテストにおけるその性能が、人間の専門家と同等（33.5% 対 33.6%）であり、一般的な人間のチーム（16.4%）を大きく上回ったことを示した。

本研究の3つの主な貢献は次のとおりである。
1）新しいベンチマークの提案：AIエージェントは、100万個の分子を含む化合物ライブラリから、最も有望な候補分子を特定する必要がある。そのためには、化学空間の探索、予測モデルの選択、複数の目標のバランス、限られた資源の管理など、戦略を自律的に策定し実行する必要があり、これは医薬品開発における複雑で制約のある意思決定環境を模倣している。
2）強力なシステムの提示：多エージェントシステムDeep Thoughtを構築し、ベンチマークテストで優れた性能を示した。また、消去研究を通じて、さまざまな役割におけるさまざまな言語モデルの性能を分析した。
3）人とAIの比較評価：このシステムと人間の参加チームおよび専門家のソリューションの性能を比較し、医薬品発見タスクにおけるAIと人間の相対的な強み、弱み、戦略の特徴を明らかにした。

DO Challengeベンチマークタスク
このベンチマークテストは、仮想スクリーニングタスクを模倣することにより、分子スクリーニング効率の向上におけるAIシステムの総合能力を体系的に評価する。エージェントは上位レベルの意思決定を行い、プログラミング、実行、デバッグなどの方法を通じて戦略の実行を実現し、計算医薬品発見におけるエンドツーエンドの自動化の実際的なニーズを模倣する。

ベンチマークタスクは、100万個の一意な分子コンフォメーションを含む固定されたデータセットに基づいている。各コンフォメーションには、薬物候補分子としての可能性を示すラベルDO Scoreが付いている。評価に参加するすべてのAIシステムは、同じデータセットを使用する。タスクの目的は、DO Scoreが最も高い上位1000個の分子構造を特定することである。初期データセットにはDO Scoreラベルが含まれていない。エージェントは、最大10万個の構造についてDO Scoreの注釈を要求でき、どの分子に注釈を付けるかは自分で決定する必要がある。一度に要求することも、複数回に分けて要求することもできる。最終的に、エージェントは最も有望な3000個の構造を評価のために提出する必要がある。評価指標は、予測された上位1000個の構造と実際の構造との重なり合いの程度である。エージェントは最大3回の提出機会があり、各提出後にはスコアフィードバックのみが得られ、どの構造が正しいかという情報は提供されない。最終的なスコアは、3回中最高の値が使用される。タスクの複雑性を確保するために、タスクの説明には、標的タンパク質に関する情報は一切提供されておらず、選択は分子自体に完全に依存する。

Deep Thoughtエージェントシステム
Deep Thoughtは、モジュール式の多エージェントシステムであり、ソフトウェアエンジニアリング、機械学習、システムアーキテクチャ、科学研究などの分野に関連する複雑なタスクを自律的に実行するように設計されている。ここでは、4つの重要なエージェントコンポーネントについて説明する。

1）Software Engineerエージェントグループ。タスク計画、コード探索、実装、レビューなどの段階を含む、全プロセスコード開発を担当する。
2）Installer＆Evaluationエージェントグループ。エージェントが独立した仮想環境を作成し、依存関係を識別し、コードを実行し、エラーを分析し、最適化の提案を提供するために使用される。Installerエージェントは、仮想環境に依存関係をインストールし、構文の問題を修正し、バージョン競合を処理する。
3）Scientistエージェント（オプション）。ユーザーの初期の問題を処理し、他のエージェントが実行する行動計画を作成する。
4）Researchエージェントグループ。関連資料を検索して収集し、重要な情報を抽出し、タスクとの関連性に基づいてそれらをソートする。
図1 Deep Thoughtアーキテクチャ

実験設定
実験1：DO Challenge
評価対象は3種類ある。20の人のチームが参加したDO Challenge 2025、さまざまなLLMの組み合わせを含むDeep Thoughtエージェントシステム、領域知識を持つ2人の人間の専門家である。

2つの単純なベースラインを設定した。
ベースライン1：10万個の分子をランダムに選択し、その中でDO Score値が最も高い3000個を提出する。次に、実際のTop 1000と重なる分子を保持し、2巡目では新しいランダムサンプルで補う。
ベースライン2：ベースライン1に基づき、2巡目はランダムに選択せず、1巡目でヒットした分子と構造的に類似した（フィンガープリントのTanimoto類似度で測定）未注釈の分子を優先的に選択する。

ベンチマークテストは、3つの設定で行われた。実際のチャレンジを模倣した10時間制限、性能の上限を探る無制限時間、時間制限を解除した後の効果の変化を評価するコンテスト後の拡張である。

実験2：Deep Thoughtの効率と安定性のテスト
Software EngineerエージェントにClaude 3.7 Sonnetを使用し、ReviewerエージェントにGPT-4oを使用する組み合わせをシステムのデフォルト設定（Deep Thought cfg-4）とした。効率と安定性におけるさまざまなLLMの差異をよりよく理解するために、全体の実行時間と使用されたLLMトークンの数について、最も優れた性能を示したいくつかのDeep Thought設定を比較した。主エージェントは、Claude 3.7 Sonnet、Gemini 2.5 Pro、o3のバージョンである。

結果と分析
DO Challenge実験結果
さまざまな設定における実験結果のランキングは次のとおりである。3つの設定はそれぞれ表1、2、3に対応している。タスクの特徴とドメイン知識を組み合わせることで、著者は今回のチャレンジタスクにおける高性能に関連する4つの主要な要因を特定し、表の後4列に対応させた。

1）構造選択戦略：アクティブラーニング、クラスタリング、または類似性ベースのスクリーニングなどの、より高度な構造選択方法を採用する。
2）空間関係ニューラルネットワーク：グラフニューラルネットワーク、アテンション機構アーキテクチャ、3次元畳み込みニューラルネットワーク、またはその変種などを使用して、分子コンフォメーションにおける空間関係と構造情報を特に対象とする。
3）位置非不変性のモデリング：構造の位置（並進と回転）に非不変な特徴を使用する。注目すべきは、タスクの説明ではDO Scoreは原子位置の変化に敏感であることが明示的に示されているが、すべての解決策がこれを十分に考慮しているわけではないということである。
4）戦略的な提出：実際のラベルとモデルの予測を組み合わせてインテリジェントに組み合わせ、最大3回の提出機会を活用し、前回の結果を使用して後続の提出を最適化する。
表1 10時間制限設定におけるDO Challengeランキング
表2 無制限時間設定におけるDO Challengeトップ15ランキング
AIと人間の性能に関して、時間制限のある条件下では、人間の専門家が最高の性能を示し、次にOpenAI o3モデルをメインエージェントとして使用したDeep Thought設定（cfg-10）が続いた。この設定で優れた性能を示した他の設定には、Claude 3.7 Sonnet（cfg-1、cfg-4）とGemini 2.5 Proをメインエージェントとして使用したDeep Thoughtの組み合わせ（cfg-2、cfg-3）が含まれる。無制限時間の設定では、人間の専門家のリードがより顕著になり、Deep Thought（cfg-10、メインモデルはo3）が3位となった。コンテスト後の拡張設定では、Deep Thought（cfg-6、メインモデルはGemini 2.5 Pro）が最高の成績を収めたが、この設定では人間の専門家の対照結果が得られず、その性能はより厳しい条件下で専門家が得た結果を下回っている。
表3 チャレンジコンテスト後の拡張結果
正式なコンテスト後、主催者は1週間のオプション拡張フェーズを開設し、時間制限を解除した後のソリューションの質への影響を評価することを目的とした。メインコンテストからの8チームが自発的に参加し、5チームが選抜され、そのうち3チームが時間内に拡張ソリューションを提出した。そのうち2チームは拡張フェーズでメインコンテストフェーズよりも良い成績を収めた。Deep Thoughtシリーズのエージェントもこのフェーズに参加し、スコアが大幅に向上した。

Deep Thoughtの効率と安定性の結果
システムのデフォルト設定（Deep Thought cfg-4）は、ベンチマークテストで5回の独立した実行において表4に示されたような性能を示した。
表4 Deep Thought cfg-4の5回の独立した実行結果
3つの異なるLLMをそれぞれメインエージェントとして使用したDeep Thoughtの全体の実行時間と使用されたLLMトークンの数の違いを図2に示す。3つのバージョンはいずれも高性能設定であり、さまざまな設定のパフォーマンス傾向と変動状況を示している。
図2 さまざまなDeep Thought設定における全体の実行時間と使用されたLLMトークンの数

Deep Thought消去結果
1）Software Engineerエージェントグループ
Software Engineerの位置で複数のLLMをテストし、ReviewerはGPT-4oのままとした。結果は表5に示されているように、o3、Claude 3.7 Sonnet、Gemini 2.5 Proの3つは他のモデルより明らかに優れている。o3は、時間制限ありと制限なしの設定の両方で最高の成績を収めた。Claude 3.5 Haikuなどの小さなモデルは完全に失敗し、実行可能なコードを生成できなかった。
表

[원문 링크](https://hub.baai.ac.cn/view/47454)
