import email
import logging
import os
import re
import base64
import threading
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

from bs4 import BeautifulSoup

from .common_utils import (
    clean_google_url,
    fetch_article_body,
    get_gmail_service,
    get_existing_english_titles_from_dir,
    safe_filename,
    translate_text,
    summarize_and_translate_body,
    initialize_gemini,
)

# --- Constants ---
SEEN_PAPERS_FILE = "seen_scholar_messages.json"

PAPERS_OUTPUT_DIR = os.path.join("docs", "keywords")
MIN_BODY_LENGTH = 300


# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)


def get_html_payload_from_message(msg):
    """Gmail API ë©”ì‹œì§€ ê°ì²´ì—ì„œ HTML payloadë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
    if "parts" in msg["payload"]:
        for part in msg["payload"]["parts"]:
            if part["mimeType"] == "text/html":
                return part["body"].get("data")
            # ì¬ê·€ í˜¸ì¶œ
            data = get_html_payload_from_message({"payload": part})
            if data:
                return data
    elif msg["payload"]["mimeType"] == "text/html":
        return msg["payload"]["body"].get("data")
    return None


def parse_scholar_email(msg):
    """Gmail ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œì™€ ë…¼ë¬¸ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    headers = {h["name"]: h["value"] for h in msg["payload"]["headers"]}
    subject = headers.get("Subject", "")

    keyword_match = re.match(r'^(.*?)\s*-\s*(?:new results|ìƒˆë¡œìš´ ê²°ê³¼)', subject, re.IGNORECASE)
    
    if not keyword_match:
        logging.info(f"ìœ íš¨í•œ ì•Œë¦¼ ë©”ì¼ì´ ì•„ë‹˜ (ì œëª©: '{subject}'). ê±´ë„ˆëœë‹ˆë‹¤.")
        return None, [] # ìœ íš¨í•œ ì•Œë¦¼ì´ ì•„ë‹ˆë¯€ë¡œ Noneì„ ë°˜í™˜í•˜ì—¬ ë²„ë¦¬ë„ë¡ ì‹ í˜¸

    # ë§¤ì¹˜ëœ ê²½ìš°, ì²« ë²ˆì§¸ ê·¸ë£¹ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    keyword = keyword_match.group(1).strip()
    logging.info(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keyword}")

    body_data = get_html_payload_from_message(msg)
    if not body_data:
        logging.error("ì´ë©”ì¼ì—ì„œ HTML ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return keyword, []

    html_content = base64.urlsafe_b64decode(body_data).decode("utf-8")
    soup = BeautifulSoup(html_content, "html.parser")
    articles = []

    # ì‹¤ì œ ì´ë©”ì¼ HTML êµ¬ì¡°ì— ë§ê²Œ, class="gse_alrt_title"ë¥¼ ê°€ì§„ <a> íƒœê·¸ë¥¼ ì§ì ‘ ì°¾ìŠµë‹ˆë‹¤.
    for link_tag in soup.find_all("a", class_="gse_alrt_title"):
        title_en = link_tag.get_text(strip=True)
        url = link_tag.get("href", "")

        # ìŠ¤ë‹ˆí«(ìš”ì•½)ì„ ì°¾ê¸° ìœ„í•´, ë¨¼ì € <a> íƒœê·¸ì˜ ë¶€ëª¨ì¸ <h3> íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        h3_tag = link_tag.find_parent("h3")
        snippet = ""
        if h3_tag:
            # <h3> íƒœê·¸ì˜ ë°”ë¡œ ë‹¤ìŒ í˜•ì œì¸ <div class="gse_alrt_sni">ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            snippet_tag = h3_tag.find_next_sibling("div", class_="gse_alrt_sni")
            if snippet_tag:
                snippet = snippet_tag.get_text(strip=True)
        
        if title_en and url:
            articles.append({"title_en": title_en, "url": url, "snippet": snippet})

    logging.info(f"ğŸ“„ ì´ë©”ì¼ì—ì„œ {len(articles)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return keyword, articles

def save_paper_markdown(keyword, title_ko, title_en, summary_ko, url):
    """ë…¼ë¬¸ ìš”ì•½ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        safe_title = safe_filename(title_ko)
        folder = os.path.join(PAPERS_OUTPUT_DIR, keyword)
        os.makedirs(folder, exist_ok=True)
        path = os.path.join(folder, f"{safe_title}.md")

        with open(path, "w", encoding="utf-8") as f:
            f.write(f"# {title_ko}\n\n")
            f.write(f"**ì›ì œëª©:** {title_en}\n\n")
            f.write(f"**ìš”ì•½:** {summary_ko}\n\n")
            f.write(f"[ì›ë¬¸ ë§í¬]({url})\n")
        logging.info(f"âœ… ì €ì¥ ì™„ë£Œ: {path}")
        return True

    except Exception as e:
        logging.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({title_en}): {e}")
        return False

def load_seen_ids():
    """ì²˜ë¦¬ëœ ì´ë©”ì¼ ID ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if os.path.exists(SEEN_PAPERS_FILE):
        with open(SEEN_PAPERS_FILE, "r") as f:
            return set(json.load(f))
    return set()


def save_seen_ids(ids):
    """ì²˜ë¦¬ëœ ì´ë©”ì¼ ID ëª©ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(SEEN_PAPERS_FILE, "w") as f:
        json.dump(list(ids), f)


def process_paper_entry(article, keyword, existing_titles, lock):
    """ê°œë³„ ë…¼ë¬¸ í•­ëª©ì„ ì²˜ë¦¬, ë²ˆì—­, ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        title_en, link_url, snippet = article["title_en"], article["url"], article["snippet"]

        with lock:
            if title_en in existing_titles:
                logging.info(f"ğŸš« ì¤‘ë³µ ë…¼ë¬¸: {title_en}")
                return False
            existing_titles.add(title_en)

        logging.info(f"--- âš™ï¸ ì²˜ë¦¬ ì‹œì‘: {title_en} ---")

        # ì–´ë–¤ URLì„ ì²˜ë¦¬í•˜ê¸° ì‹œì‘í–ˆëŠ”ì§€ ì¦‰ì‹œ ë¡œê¹…í•˜ì—¬, ë©ˆì¶¤ í˜„ìƒì˜ ì›ì¸ì„ ì¶”ì í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        logging.info(f"  -> URL: {link_url}")

        link = clean_google_url(link_url)

        # 1. ë³¸ë¬¸ ì¶”ì¶œì„ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.
        body = fetch_article_body(link)

        # 2. ë³¸ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´, ì´ë©”ì¼ì˜ ìŠ¤ë‹ˆí«ì„ ëŒ€ì²´ì¬ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if not body or len(body.strip()) < MIN_BODY_LENGTH:
            logging.info(f"  - â„¹ï¸ ì •ë³´: ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±. ì´ë©”ì¼ ìŠ¤ë‹ˆí«ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            body = snippet

        # 3. ë³¸ë¬¸ê³¼ ìŠ¤ë‹ˆí«ì´ ëª¨ë‘ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
        if not body or not body.strip():
            logging.warning(f"ë³¸ë¬¸/ìŠ¤ë‹ˆí«ì´ ëª¨ë‘ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤: {title_en}")
            return False

        logging.info(f"  -> ì œëª© ë²ˆì—­ ì¤‘...")
        title_ko = translate_text(title_en)
        logging.info(f"  -> ì œëª© ë²ˆì—­ ì™„ë£Œ.")

        logging.info(f"  -> ë³¸ë¬¸ ìš”ì•½ ë° ë²ˆì—­ ì¤‘...")
        summary_ko = summarize_and_translate_body(body)
        logging.info(f"  -> ë³¸ë¬¸ ìš”ì•½ ë° ë²ˆì—­ ì™„ë£Œ.")

        if not title_ko or not summary_ko:
            logging.error(f"ë²ˆì—­ ì‹¤íŒ¨: {title_en}")
            return False
        
        logging.info(f"  -> ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì¤‘...")
        if save_paper_markdown(keyword, title_ko, title_en, summary_ko, link):
            return True # ì œëª©ì€ ì´ë¯¸ ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.
        return False
    except Exception as e:
        logging.error(f"ë…¼ë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({article.get('title_en', 'N/A')}): {e}")
        return False

def main():
    try:
        initialize_gemini()
    except (ValueError, RuntimeError) as e:
        logging.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ë‹¨: {e}")
        # CI/CD í™˜ê²½ì—ì„œ ì‹¤íŒ¨ë¥¼ ëª…í™•íˆ ì•Œë¦¬ê¸° ìœ„í•´ 0ì´ ì•„ë‹Œ ì½”ë“œë¡œ ì¢…ë£Œ
        exit(1)

    service = get_gmail_service()
    if not service:
        # get_gmail_service ë‚´ë¶€ì—ì„œ ì´ë¯¸ CRITICAL ì—ëŸ¬ë¥¼ ë¡œê¹…í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨ì‹œí‚µë‹ˆë‹¤.
        raise RuntimeError("Gmail ì„œë¹„ìŠ¤ ê°ì²´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

    query = "from:scholaralerts-noreply@google.com is:unread"
    logging.info(f"ğŸ” Gmailì—ì„œ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ ìƒˆ ë…¼ë¬¸ ì•Œë¦¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤: '{query}'")
    results = service.users().messages().list(userId="me", q=query).execute()
    messages = results.get("messages", [])

    if not messages:
        # ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œë„ ì„±ê³µì ìœ¼ë¡œ í™•ì¸í–ˆë‹¤ëŠ” ì˜ë¯¸ë¡œ âœ… ì•„ì´ì½˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        logging.info("âœ… ì²˜ë¦¬í•  ìƒˆ Google Scholar ì•Œë¦¬ë¯¸ ë©”ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ì´ {len(messages)}ê°œì˜ ìƒˆ ì•Œë¦¬ë¯¸ ë©”ì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    # ëª¨ë“  ê³µìœ  ìì›ì„ ë£¨í”„ ì‹œì‘ ì „ì— ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    seen_ids = load_seen_ids()
    successful_saves = 0
    all_paper_tasks = []
    existing_titles_cache = {}  # í‚¤ì›Œë“œë³„ ê¸°ì¡´ ì œëª© ìºì‹œ
    lock = threading.Lock()

    # 1ë‹¨ê³„: ëª¨ë“  ì´ë©”ì¼ì„ ìˆœíšŒí•˜ë©° ì²˜ë¦¬í•  ì‘ì—… ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    for msg_info in messages:
        msg_id = msg_info["id"]
        if msg_id in seen_ids:
            continue
        
        try:
            msg = service.users().messages().get(userId="me", id=msg_id, format="full").execute()
            keyword, articles = parse_scholar_email(msg)
            
            if keyword and articles: # í‚¤ì›Œë“œì™€ ë…¼ë¬¸ì´ ëª¨ë‘ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                # ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ í‚¤ì›Œë“œì— ëŒ€í•´ íŒŒì¼ ì‹œìŠ¤í…œì„ ë°˜ë³µì ìœ¼ë¡œ ì½ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
                if keyword not in existing_titles_cache:
                    keyword_dir = os.path.join(PAPERS_OUTPUT_DIR, keyword)
                    existing_titles_cache[keyword] = get_existing_english_titles_from_dir(keyword_dir)
                
                # í˜„ì¬ ì´ë©”ì¼ì˜ ëª¨ë“  ë…¼ë¬¸ì„ ì „ì²´ ì‘ì—… ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                for article in articles:
                    all_paper_tasks.append((article, keyword, existing_titles_cache[keyword], lock))
                logging.info(f"ì´ë©”ì¼ ID {msg_id}ì—ì„œ {len(articles)}ê°œì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬ ëŒ€ê¸°ì—´ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

            # ì´ë©”ì¼ í™•ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚˜ë©´, ìœ íš¨í•œ ë°ì´í„° ìœ ë¬´ì™€ ìƒê´€ì—†ì´ 'ì½ìŒ' ì²˜ë¦¬í•˜ì—¬ ë‹¤ì‹œ í™•ì¸í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
            service.users().messages().modify(userId="me", id=msg_id, body={"removeLabelIds": ["UNREAD"]}).execute()
            seen_ids.add(msg_id)

        except Exception as e:
            logging.error(f"ì´ë©”ì¼ ID {msg_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ, í•´ë‹¹ ì´ë©”ì¼ì€ 'ì½ìŒ' ì²˜ë¦¬í•˜ì§€ ì•Šê³  ê±´ë„ˆë›°ì–´ ë‹¤ìŒ ì‹¤í–‰ ë•Œ ì¬ì‹œë„í•˜ë„ë¡ í•©ë‹ˆë‹¤.
            continue

    # 2ë‹¨ê³„: ìˆ˜ì§‘ëœ ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    if all_paper_tasks:
        logging.info(f"ì´ {len(all_paper_tasks)}ê°œì˜ ë…¼ë¬¸ í•­ëª©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_task = {executor.submit(process_paper_entry, task[0], task[1], task[2], task[3]): task for task in all_paper_tasks}
            count = 0
            total = len(future_to_task)
            for future in as_completed(future_to_task):
                count += 1
                logging.info(f"  - ë…¼ë¬¸ ì§„í–‰ë¥ : {count}/{total} ì²˜ë¦¬ ì™„ë£Œ...")
                try:
                    if future.result() is True:
                        successful_saves += 1
                except Exception as exc:
                    logging.error(f"ë…¼ë¬¸ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")

    logging.info(f"========== Google Scholar ìˆ˜ì§‘ ì¢…ë£Œ: ì´ {successful_saves}ê°œì˜ ìƒˆ ë…¼ë¬¸ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ==========\n")
    
    # 3ë‹¨ê³„: ëª¨ë“  ì‘ì—…ì´ ëë‚œ í›„, ì²˜ë¦¬ëœ ID ëª©ë¡ì„ ìµœì¢… ì €ì¥í•©ë‹ˆë‹¤.
    save_seen_ids(seen_ids)


if __name__ == "__main__":
    main()